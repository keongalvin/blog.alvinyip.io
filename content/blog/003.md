+++
title = "Lazy file IO in Python"
date = "2023-09-19T20:27:37+08:00"

#
# description is optional
#
# description = "An optional description for SEO. If not provided, an automatically created summary will be used."

tags = []
+++

## Motivation 

When we want to read a large file, we could either

1. load the entire file in memory
2. iterate over the contents of the file and work chunk by chunk

For sake of discussion suppose I want to copy the file and save it into another location. Here is how approach 1 would look like

```python
from pathlib import Path

source = Path("/path/to/source")
dest = Path("/path/to/destination")
with open(source, "rb") as s, open(dest, "wb+") as d:
    d.write(s.read())
```

This approach will usually no longer work if the file is too large (>32GiB). Instead we can read only a portion of the file, and write it into the other file. Here is an example

```python
from pathlib import Path

CHUNK_SIZE = 64 * 1024 * 1024 # 64 MiB
with open(source, "rb") as s, open(dest, "wb") as d:
    while chunk := s.read(CHUNK_SIZE):
        d.write(chunk)
```

The `open` [^1] function in `[b]inary` read mode returns a `io.BufferedIOReader`, and s.`read` returns `b''` if there are no more contents in the file. A `io.BufferedIOReader` has a way of "knowing" where it currently is[^2]. Common in all `io` objects, this is Python's version of a _stream_.

Thinking in such terms not only allows for efficient usage of memory, but also generalizes to anything that can is _file-like_. 


## Example: Daisy Chain Copying

I use this most often in interacting with cloud provider blob storage APIs - here is a small implementation of "daisy chain"[^3] copying in Python for both GCP and AWS Python SDKs.

```python
from google.cloud import storage as gcs
from google.cloud.storage.fileio import BlobWriter, BlobReader, DEFAULT_CHUNK_SIZE

client = gcs.Client()
bucket = client.get_bucket("bucket-name")
source = bucket.get_blob("source-blob-name")
destination = bucket.blob("destination-blob-name")
with source.open("rb") as s, destination.open("wb") as d:
    s: BlobReader
    d: BlobWriter
    while chunk := s.read(DEFAULT_CHUNK_SIZE):
        d.write(chunk)
```
 
In place of `io.BufferedIOReader` or `io.BufferedIOWriter`, we have `BlobReader` and `BlobWriter` respectively. These writers wrap the underlying Google Cloud Storage (GCS) APIs, so that read/write indeed writes to GCS. Unlike GCS SDK, the AWS SDK (`boto3`) does not clearly expose a mechaism to stream objects; instead we will use `s3fs`[^4] library to provide a file-like interface. 

```python
from s3fs import S3FileSystem

s3 = S3FileSystem(anon=False)
bucket_name = "bucket-name"
source_key = "source-key"
destination_key = f"destination-key"
source_path = f"{bucket_name}/{source_key}"
destination_path = f"{bucket_name}/{destination_key}"
with s3.open(source_path, mode='rb') as s, s3.open(destination_path, mode="wb") as d:
    while chunk := s.read(1024):
        d.write(chunk)
```



## Footnotes

[^1]: https://docs.python.org/3.11/library/functions.html#open.
[^2]: See the `tell` method in https://docs.python.org/3.11/library/io.html#io.IOBase.
[^3]: refer to the `-D` option in https://cloud.google.com/storage/docs/gsutil/commands/cp#options
[^4]: https://github.com/fsspec/s3fs