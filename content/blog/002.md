+++
title = "Exploring the world of GenAI Apps"
date = "2023-08-18"

# Set menu to "main" to add this page to
# the main menu on top of the page
# menu = "main"

# Add optional description for SEO. If not provided, 
# an automatically created summary will be used.
#
# tags are optional
# tags = ["markdown","syntax",]

+++

So I finally decided to build an app that uses Generative AI - I'm sharing my experiences here.

### Motivation

Hype. Excitement. Curiosity. What does it take to build one of those apps? Could I do it?

### Objectives
I think it's important to focus on the core outcomes I would like to achieve:

1. Have fun learning new tools/technologies.
2. Build something useful.
 
After some research I decided to focus on building a 
retrieval augmented generation application. The idea being that by providing LLM with suitable context 
and phrasing the prompt appropriately, the LLM would generate the most appropriate response.

The flow is as such

1. Tokenize a bunch of documents and store this information in a vectorstore.
2. When app receives a prompt from user, identify most similar documents via a similarity search. Use this as context and pass it to the LLM.
3. LLM with context should respond intelligently to the query.


### The Journey 

#### Part 1 - MVP of MVPs

Without data there no document to search on, so I looked at how to get this data. 
The data engineering process is important, but for the purpose of this app I'm not that interested. 
Looking to yak shave[^1], I followed this [tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html) replacing the url with https://books.toscrape.com/.
This gives me a folder with some HTML files. To tokenize them I used [sentence-transformers](https://www.sbert.net/)

```python
from sentence_transformers import SentenceTransformer
from pathlib import Path

MODEL_NAME = 'all-distilroberta-v1'
DATA_DIRECTORY = Path('path/to/data') 

model = SentenceTransformer(MODEL_NAME)
texts = [] # list of tuple (filename, filecontents)

for file in DATA_DIRECTORY.glob("**/index.html"):
    name = file.parts[-2]
    print(f"reading {name}")
    text = file.read_text()
    texts.append((name, text))
    print(f"loaded {len(texts)} pages")

embeddings = model.encode(list(map(lambda x: x[1], texts)))
```

Now I'll have to store this in a vectorstore. I've seen [pgvector](https://github.com/pgvector/pgvector)
earlier, so I quickly set up a docker-compose.yaml[^2] with the `ankane/pgvector` image. Now that I have pgvector set up I'll just need to insert the vectors and text into the table.

```python
import psycopg
from pgvector.psycopg import register_vector

TABLE_NAME = "vectors"

with psycopg.connect(CONNECTION_STRING) as conn:
    register_vector(conn)
    conn.execute(
            f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            id bigserial PRIMARY KEY,
            filename VARCHAR(512) NOT NULL,
            data BYTEA NOT NULL,
            embedding vector(768) NOT NULL
        );
        """
        )

    for filename_data, emb in zip(texts, embeddings):
        filename, data = filename_data
        conn.execute(
            """
        INSERT INTO items (filename, data, embedding)
            VALUES (%(filename)s, %(data)s, %(embedding)s);
        """,
            {"filename": filename, "data": data.encode("utf-8"), "embedding": emb},
        )

``` 

To get the most relevant book we can do something like

```python
prompt = "interesting book about space exploration"
embedding = model.encode(prompt)

with psycopg.connect(CONNECTION_STRING) as conn:
    register_vector(conn)
    
    result = conn.execute(f"SELECT filename FROM {TABLE_NAME} ORDER BY embedding <-> %(embedding)s LIMIT 1",
                          {"embedding": embedding}).fetchall()
    print(result)
```
So if I want an interesting book about space exploration I should read [mesaerion-the-best-science-fiction-stories](http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/).

This is pretty neat; just need to glue them all together and stick a fancy UI, right? 

#### Part 2 - Python/LangChain

[LangChain](https://www.langchain.com/) is a popular framework to build LLM apps with. I found the conceptual breakdown of the various 
components (Chains, Memory, Agents, Model I/O) to be extremely useful in understanding what we are trying to achieve.

To load data we define a `vectorstore` from documents, which is loaded via a `DocumentLoader`.
Document transformations are also possible, and loading into postgres is equally simple 
This is basically an ETL pipeline, from raw html to vectors.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader
from langchain.document_transformers import Html2TextTransformer
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import PGVector

documents = DirectoryLoader(DATA_DIRECTORY, glob="**/*.html").load()
text_splitter = RecursiveCharacterTextSplitter()
html2text = Html2TextTransformer()
documents = html2text.transform_documents(documents=documents)
documents = text_splitter.split_documents(documents)


embedding = SentenceTransformerEmbeddings(model_name=MODEL_NAME)
vectorstore = PGVector.from_documents(
            documents=documents,
            embedding=embedding,
            connection_string=CONNECTION_STRING,
        )
```
The nice thing about using this is that metadata is compatible with the subsequent steps.
In now to get a response/question we can define a `chain` - this is callable i.e. has a `__call__` method, so we can use it like a function.
RetrievalQA[^3] is a common type of chain that is used to perform question answering - basically our use case
```python
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain.llms import VertexAI

llm = VertexAI()
prompt = "interesting book about space exploration"
chain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorstore.as_retriever(search_kwargs={'k': 5}),
        chain_type="map_rerank",
        chain_type_kwargs={
            "prompt": PROMPT,
        },
        return_source_documents=True,
    )
response = chain({"query": prompt})
```
Notes:

1. LLM is swappable - here I am using VertexAI because I have some GCP credits. There a ton of other integrations too.[^4]
2. to better understand where the `search_kwargs` go I went spelunking - by carefully tracing the method calls I learn that
`search_kwargs` will become a property of the `retriever` which has a `vectorstore` property and when searching 
for relevant documents the `_get_relevant_documents` method will be called. It is here that the `search_kwargs` will be used.  
Some might call this elementary debugging, but I thought it was useful to share the process.   

3. `PROMPT` is a `langchain.prompts.PromptTemplate`. Here is where the prompt engineering piece takes place. 
I found the need to hack the parser because for unknown reasons the model is not giving me responses in the desired format. 
I am sure with sufficient tuning or better prompt this could change, your mileage may vary.

```python
from langchain.output_parsers.regex import RegexParser
from langchain.prompts import PromptTemplate

class LaxRegexParser(RegexParser):
    def parse(self, text: str) -> dict[str, str]:
        try:
            return super().parse(text)
        except ValueError:
            return {"answer": "I gave you an empty string", "score": -1}


output_parser = LaxRegexParser(
    regex=r"(.*?)\nScore: (\d*)",
    output_keys=["answer", "score"],
)

prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:

Question: [question here]
Helpful Answer: [answer here]
Score: [score between 0 and 100]

How to determine the score:
- Higher is a better answer
- Better responds fully to the asked question, with sufficient level of detail
- If you do not know the answer based on the context, that should be a score of 0
- Don't be overconfident!
- YOU MUST ADHERE TO THIS FORMAT FOR ALL RESPONSES! 

Example #1

Context:
---------
Apples are red
---------
Question: what color are apples?
Helpful Answer: red
Score: 100

Example #2

Context:
---------
it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv
---------
Question: what type was the car?
Helpful Answer: a sports car or an suv
Score: 60

Example #3

Context:
---------
Pears are either red or orange
---------
Question: what color are apples?
Helpful Answer: This document does not answer the question
Score: 0

Begin!

Context:
---------
{context}
---------
Question: {question}
Helpful Answer:"""
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"],
    output_parser=output_parser,
)
```
So having build the chain we can simply use it in our application. But this is still quite far from the typical 
ChatGPT lite application, with a nice user interface etc. So I found out that LangChain.js[^5] is a thing so time
to go a little deeper.

#### Part 3 - JS Rabbit Hole

I have no experience writing javascript but this is likely the shortest path, plus I've wanted an ~~excuse~~ opportunity to use Svelte, and there seems to be a pretty useful starting point[^6], so I went into it and oh boy did I get myself into a rabbit hole. So much for yak shaving earlier.

Since I've already had data stored in postgres I thought of using one of the available integrations - perhaps 
[TypeORM](https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/typeorm)?

Turns out there are some column mapping issues[^7] and so instead of mucking around with SQL and transforming columns I tried to load the documents in javascript.
However, there doesn't seem to be a `SentenceTransformer` equivalent in JS world, so I changed the embedding to use `GoogleVertexAIEmbeddings`. 

Now before I run the application it seems like there is a dependency on [Vercel KV](https://vercel.com/storage/kv).
Looking to avoid this I found [serverless-redis-http](https://github.com/hiett/serverless-redis-http/tree/master), 
which is a drop in replacement for `@upstash/redis` which `@vercel/kv` uses under the hood. This means that by running it I could avoid the dependency on Vercel KV, so two new containers joined the docker-compose setup.  

Now the only way I know how to trigger the "loading" is via a render of the page. This is obviously not ideal, so I tried to
run it like a script, but turns out running typescript file on its own is actually impossible[^8]. I didn't want to mess around with `ts-node` either, so I had an idea to use [Milvus](https://milvus.io/) a vector database - since this is free of ORMs, and both languages has integrations I thought it was worth a shot.

Since I am a beginner at Milvus, I looked for a GUI so that it's easier to visualize the collections/records and indeed I found one - [attu](https://github.com/zilliztech/attu). Milvus requires `etcd`, `minio` and the `milvus` container itself, along with `attu` my docker-compose setup has a total of 6 containers.  The starter code uses [Vercel's `ai` SDK](https://sdk.vercel.ai/docs) which has integrations with LangChain.js, so I tried porting the python version of the RetrievalQA chain and fitting it into the required stream. Due to skill issue I have no idea how to use the `LangChainStream` object to accomplish what I want, so I referred to an example in the [langchain-nextjs-template](https://github.com/langchain-ai/langchain-nextjs-template/blob/add43ac/app/api/chat/retrieval/route.ts). 

Even though the language is different, the core ideas of `chain` and piping outputs into another are similar. There is a concept of a `RunnableSequence` which is an array of `Runnables`, this can be used to control the flow of execution. For example if we want to `console.log` the output we can do something similar to this

```ts
import type { AIMessage } from 'langchain/schema';
import { RunnablePassthrough, RunnableSequence } from 'langchain/schema/runnable';

const debugChain = async (completion: AIMessage) => {
   console.log(`completion: ${completion.content}`);
   return completion;
};
const answerChain = RunnableSequence.from([
   {
      context: retriever.pipe(combineDocumentsFn),
      question: new RunnablePassthrough()
   },
   ANSWER_PROMPT,
   model,
   debugChain,
   new BytesOutputParser()
]);
```
Streaming is kinda broken here but the end result is pretty neat.

### Closing

I think I've only scratched the surface of what it takes to successfully build operate and evolve a GenAI app. I really like how the individual pieces are composed but the implementations could be better improved. If I were to put this into production, python is probably not a language I would choose. 

### Footnotes

[^1]: https://americanexpress.io/yak-shaving/.
[^2]: Something similar to [this](https://github.com/khezen/compose-postgres/blob/master/docker-compose.yml), remember to run `CREATE EXTENSION vector;`
[^3]: https://python.langchain.com/docs/use_cases/question_answering
[^4]: https://python.langchain.com/docs/integrations/llms/
[^5]: https://js.langchain.com/docs/get_started/introduction
[^6]: https://github.com/jianyuan/sveltekit-ai-chatbot/tree/main
[^7]: Column names created by python version is snake_cased but the one in TypeORM uses camelCase. I am pretty sure it's possible to configure this, just too lazy to try.
[^8]: I am aware typescript is compiled down to javascript but this didn't hit me until I actually tried to "run" a typescript file like a script 